{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EchoNet-Dynamic Dataset.\"\"\"\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "class Echo(torchvision.datasets.VisionDataset):\n",
    "   \n",
    "\n",
    "    def __init__(self, root=None,\n",
    "                 split=\"train\", target_type=\"EF\",\n",
    "                 mean=0., std=1.,\n",
    "                 length=16, period=2,\n",
    "                 max_length=250,\n",
    "                 clips=1,\n",
    "                 pad=None,\n",
    "                 noise=None,\n",
    "                 target_transform=None,\n",
    "                 external_test_location=None):\n",
    "        if root is None:\n",
    "            root = \"/content/EchoNet-Dynamic\"\n",
    "\n",
    "        super().__init__(root, target_transform=target_transform)\n",
    "\n",
    "        self.split = split.upper()\n",
    "        if not isinstance(target_type, list):\n",
    "            target_type = [target_type]\n",
    "        self.target_type = target_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.length = length\n",
    "        self.max_length = max_length\n",
    "        self.period = period\n",
    "        self.clips = clips\n",
    "        self.pad = pad\n",
    "        self.noise = noise\n",
    "        self.target_transform = target_transform\n",
    "        self.external_test_location = external_test_location\n",
    "\n",
    "        self.fnames, self.outcome = [], []\n",
    "\n",
    "        if self.split == \"EXTERNAL_TEST\":\n",
    "            self.fnames = sorted(os.listdir(self.external_test_location))\n",
    "        else:\n",
    "            # Load video-level labels\n",
    "            with open(os.path.join(self.root, \"FileList.csv\")) as f:\n",
    "                data = pandas.read_csv(f)\n",
    "            data[\"Split\"].map(lambda x: x.upper())\n",
    "\n",
    "            if self.split != \"ALL\":\n",
    "                data = data[data[\"Split\"] == self.split]\n",
    "\n",
    "            self.header = data.columns.tolist()\n",
    "            self.fnames = data[\"FileName\"].tolist()\n",
    "            self.fnames = [fn + \".avi\" for fn in self.fnames if os.path.splitext(fn)[1] == \"\"]  # Assume avi if no suffix\n",
    "            self.outcome = data.values.tolist()\n",
    "\n",
    "            # Check that files are present\n",
    "            missing = set(self.fnames) - set(os.listdir(os.path.join(self.root, \"Videos\")))\n",
    "            if len(missing) != 0:\n",
    "                print(\"{} videos could not be found in {}:\".format(len(missing), os.path.join(self.root, \"Videos\")))\n",
    "                for f in sorted(missing):\n",
    "                    print(\"\\t\", f)\n",
    "                raise FileNotFoundError(os.path.join(self.root, \"Videos\", sorted(missing)[0]))\n",
    "\n",
    "            # Load traces\n",
    "            self.frames = collections.defaultdict(list)\n",
    "            self.trace = collections.defaultdict(_defaultdict_of_lists)\n",
    "\n",
    "            with open(os.path.join(self.root, \"VolumeTracings.csv\")) as f:\n",
    "                header = f.readline().strip().split(\",\")\n",
    "                assert header == [\"FileName\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"Frame\"]\n",
    "\n",
    "                for line in f:\n",
    "                    filename, x1, y1, x2, y2, frame = line.strip().split(',')\n",
    "                    x1 = float(x1)\n",
    "                    y1 = float(y1)\n",
    "                    x2 = float(x2)\n",
    "                    y2 = float(y2)\n",
    "                    frame = int(frame)\n",
    "                    if frame not in self.trace[filename]:\n",
    "                        self.frames[filename].append(frame)\n",
    "                    self.trace[filename][frame].append((x1, y1, x2, y2))\n",
    "            for filename in self.frames:\n",
    "                for frame in self.frames[filename]:\n",
    "                    self.trace[filename][frame] = np.array(self.trace[filename][frame])\n",
    "\n",
    "            # A small number of videos are missing traces; remove these videos\n",
    "            keep = [len(self.frames[f]) >= 2 for f in self.fnames]\n",
    "            self.fnames = [f for (f, k) in zip(self.fnames, keep) if k]\n",
    "            self.outcome = [f for (f, k) in zip(self.outcome, keep) if k]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find filename of video\n",
    "        if self.split == \"EXTERNAL_TEST\":\n",
    "            video = os.path.join(self.external_test_location, self.fnames[index])\n",
    "        elif self.split == \"CLINICAL_TEST\":\n",
    "            video = os.path.join(self.root, \"ProcessedStrainStudyA4c\", self.fnames[index])\n",
    "        else:\n",
    "            video = os.path.join(self.root, \"Videos\", self.fnames[index])\n",
    "\n",
    "        # Load video into np.array\n",
    "        video = loadvideo(video).astype(np.float32)\n",
    "\n",
    "        # Add simulated noise (black out random pixels)\n",
    "        # 0 represents black at this point (video has not been normalized yet)\n",
    "        if self.noise is not None:\n",
    "            n = video.shape[1] * video.shape[2] * video.shape[3]\n",
    "            ind = np.random.choice(n, round(self.noise * n), replace=False)\n",
    "            f = ind % video.shape[1]\n",
    "            ind //= video.shape[1]\n",
    "            i = ind % video.shape[2]\n",
    "            ind //= video.shape[2]\n",
    "            j = ind\n",
    "            video[:, f, i, j] = 0\n",
    "\n",
    "        # Apply normalization\n",
    "        if isinstance(self.mean, (float, int)):\n",
    "            video -= self.mean\n",
    "        else:\n",
    "            video -= self.mean.reshape(3, 1, 1, 1)\n",
    "\n",
    "        if isinstance(self.std, (float, int)):\n",
    "            video /= self.std\n",
    "        else:\n",
    "            video /= self.std.reshape(3, 1, 1, 1)\n",
    "\n",
    "        # Set number of frames\n",
    "        c, f, h, w = video.shape\n",
    "        if self.length is None:\n",
    "            # Take as many frames as possible\n",
    "            length = f // self.period\n",
    "        else:\n",
    "            # Take specified number of frames\n",
    "            length = self.length\n",
    "\n",
    "        if self.max_length is not None:\n",
    "            # Shorten videos to max_length\n",
    "            length = min(length, self.max_length)\n",
    "\n",
    "        if f < length * self.period:\n",
    "            # Pad video with frames filled with zeros if too short\n",
    "            # 0 represents the mean color (dark grey), since this is after normalization\n",
    "            video = np.concatenate((video, np.zeros((c, length * self.period - f, h, w), video.dtype)), axis=1)\n",
    "            c, f, h, w = video.shape  # pylint: disable=E0633\n",
    "\n",
    "        if self.clips == \"all\":\n",
    "            # Take all possible clips of desired length\n",
    "            start = np.arange(f - (length - 1) * self.period)\n",
    "        else:\n",
    "            # Take random clips from video\n",
    "            start = np.random.choice(f - (length - 1) * self.period, self.clips)\n",
    "\n",
    "        # Gather targets\n",
    "        target = []\n",
    "        for t in self.target_type:\n",
    "            key = self.fnames[index]\n",
    "            if t == \"Filename\":\n",
    "                target.append(self.fnames[index])\n",
    "            elif t == \"LargeIndex\":\n",
    "                # Traces are sorted by cross-sectional area\n",
    "                # Largest (diastolic) frame is last\n",
    "                target.append(np.int64(self.frames[key][-1]))\n",
    "            elif t == \"SmallIndex\":\n",
    "                # Largest (diastolic) frame is first\n",
    "                target.append(np.int64(self.frames[key][0]))\n",
    "            elif t == \"LargeFrame\":\n",
    "                target.append(video[:, self.frames[key][-1], :, :])\n",
    "            elif t == \"SmallFrame\":\n",
    "                target.append(video[:, self.frames[key][0], :, :])\n",
    "            elif t in [\"LargeTrace\", \"SmallTrace\"]:\n",
    "                if t == \"LargeTrace\":\n",
    "                    t = self.trace[key][self.frames[key][-1]]\n",
    "                else:\n",
    "                    t = self.trace[key][self.frames[key][0]]\n",
    "                x1, y1, x2, y2 = t[:, 0], t[:, 1], t[:, 2], t[:, 3]\n",
    "                x = np.concatenate((x1[1:], np.flip(x2[1:])))\n",
    "                y = np.concatenate((y1[1:], np.flip(y2[1:])))\n",
    "\n",
    "                r, c = skimage.draw.polygon(np.rint(y).astype(np.int64), np.rint(x).astype(np.int64), (video.shape[2], video.shape[3]))\n",
    "                mask = np.zeros((video.shape[2], video.shape[3]), np.float32)\n",
    "                mask[r, c] = 1\n",
    "                target.append(mask)\n",
    "            else:\n",
    "                if self.split == \"CLINICAL_TEST\" or self.split == \"EXTERNAL_TEST\":\n",
    "                    target.append(np.float32(0))\n",
    "                else:\n",
    "                    target.append(np.float32(self.outcome[index][self.header.index(t)]))\n",
    "\n",
    "        if target != []:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "        # Select clips from video\n",
    "        video = tuple(video[:, s + self.period * np.arange(length), :, :] for s in start)\n",
    "        if self.clips == 1:\n",
    "            video = video[0]\n",
    "        else:\n",
    "            video = np.stack(video)\n",
    "\n",
    "        if self.pad is not None:\n",
    "            # Add padding of zeros (mean color of videos)\n",
    "            # Crop of original size is taken out\n",
    "            # (Used as augmentation)\n",
    "            c, l, h, w = video.shape\n",
    "            temp = np.zeros((c, l, h + 2 * self.pad, w + 2 * self.pad), dtype=video.dtype)\n",
    "            temp[:, :, self.pad:-self.pad, self.pad:-self.pad] = video  # pylint: disable=E1130\n",
    "            i, j = np.random.randint(0, 2 * self.pad, 2)\n",
    "            video = temp[:, :, i:(i + h), j:(j + w)]\n",
    "\n",
    "        return video, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Additional information to add at end of __repr__.\"\"\"\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "def _defaultdict_of_lists():\n",
    "    \"\"\"Returns a defaultdict of lists.\n",
    "\n",
    "    This is used to avoid issues with Windows (if this function is anonymous,\n",
    "    the Echo dataset cannot be used in a dataloader).\n",
    "    \"\"\"\n",
    "\n",
    "    return collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions for videos, plotting and computing performance metrics.\"\"\"\n",
    "\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import cv2  # pytype: disable=attribute-error\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def loadvideo(filename: str) -> np.ndarray:\n",
    "    \"\"\"Loads a video from a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): filename of video\n",
    "\n",
    "    Returns:\n",
    "        A np.ndarray with dimensions (channels=3, frames, height, width). The\n",
    "        values will be uint8's ranging from 0 to 255.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Could not find `filename`\n",
    "        ValueError: An error occurred while reading the video\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(filename)\n",
    "    capture = cv2.VideoCapture(filename)\n",
    "\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    v = np.zeros((frame_count, frame_height, frame_width, 3), np.uint8)\n",
    "\n",
    "    for count in range(frame_count):\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            raise ValueError(\"Failed to load frame #{} of {}.\".format(count, filename))\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        v[count, :, :] = frame\n",
    "\n",
    "    v = v.transpose((3, 0, 1, 2))\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def savevideo(filename: str, array: np.ndarray, fps: typing.Union[float, int] = 1):\n",
    "    \"\"\"Saves a video to a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): filename of video\n",
    "        array (np.ndarray): video of uint8's with shape (channels=3, frames, height, width)\n",
    "        fps (float or int): frames per second\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    c, _, height, width = array.shape\n",
    "\n",
    "    if c != 3:\n",
    "        raise ValueError(\"savevideo expects array of shape (channels=3, frames, height, width), got shape ({})\".format(\", \".join(map(str, array.shape))))\n",
    "    fourcc = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')\n",
    "    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in array.transpose((1, 2, 3, 0)):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "\n",
    "def get_mean_and_std(dataset: torch.utils.data.Dataset,\n",
    "                     samples: int = 128,\n",
    "                     batch_size: int = 8,\n",
    "                     num_workers: int = 4):\n",
    "    \"\"\"Computes mean and std from samples from a Pytorch dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): A Pytorch dataset.\n",
    "            ``dataset[i][0]'' is expected to be the i-th video in the dataset, which\n",
    "            should be a ``torch.Tensor'' of dimensions (channels=3, frames, height, width)\n",
    "        samples (int or None, optional): Number of samples to take from dataset. If ``None'', mean and\n",
    "            standard deviation are computed over all elements.\n",
    "            Defaults to 128.\n",
    "        batch_size (int, optional): how many samples per batch to load\n",
    "            Defaults to 8.\n",
    "        num_workers (int, optional): how many subprocesses to use for data\n",
    "            loading. If 0, the data will be loaded in the main process.\n",
    "            Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "       A tuple of the mean and standard deviation. Both are represented as np.array's of dimension (channels,).\n",
    "    \"\"\"\n",
    "\n",
    "    if samples is not None and len(dataset) > samples:\n",
    "        indices = np.random.choice(len(dataset), samples, replace=False)\n",
    "        dataset = torch.utils.data.Subset(dataset, indices)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    n = 0  # number of elements taken (should be equal to samples by end of for loop)\n",
    "    s1 = 0.  # sum of elements along channels (ends up as np.array of dimension (channels,))\n",
    "    s2 = 0.  # sum of squares of elements along channels (ends up as np.array of dimension (channels,))\n",
    "    for (x, *_) in tqdm.tqdm(dataloader):\n",
    "        x = x.transpose(0, 1).contiguous().view(3, -1)\n",
    "        n += x.shape[1]\n",
    "        s1 += torch.sum(x, dim=1).numpy()\n",
    "        s2 += torch.sum(x ** 2, dim=1).numpy()\n",
    "    mean = s1 / n  # type: np.ndarray\n",
    "    std = np.sqrt(s2 / n - mean ** 2)  # type: np.ndarray\n",
    "\n",
    "    mean = mean.astype(np.float32)\n",
    "    std = std.astype(np.float32)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def bootstrap(a, b, func, samples=10000):\n",
    "    \"\"\"Computes a bootstrapped confidence intervals for ``func(a, b)''.\n",
    "\n",
    "    Args:\n",
    "        a (array_like): first argument to `func`.\n",
    "        b (array_like): second argument to `func`.\n",
    "        func (callable): Function to compute confidence intervals for.\n",
    "            ``dataset[i][0]'' is expected to be the i-th video in the dataset, which\n",
    "            should be a ``torch.Tensor'' of dimensions (channels=3, frames, height, width)\n",
    "        samples (int, optional): Number of samples to compute.\n",
    "            Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "       A tuple of (`func(a, b)`, estimated 5-th percentile, estimated 95-th percentile).\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "\n",
    "    bootstraps = []\n",
    "    for _ in range(samples):\n",
    "        ind = np.random.choice(len(a), len(a))\n",
    "        bootstraps.append(func(a[ind], b[ind]))\n",
    "    bootstraps = sorted(bootstraps)\n",
    "\n",
    "    return func(a, b), bootstraps[round(0.05 * len(bootstraps))], bootstraps[round(0.95 * len(bootstraps))]\n",
    "\n",
    "\n",
    "def latexify():\n",
    "    \"\"\"Sets matplotlib params to appear more like LaTeX.\n",
    "\n",
    "    Based on https://nipunbatra.github.io/blog/2014/latexify.html\n",
    "    \"\"\"\n",
    "    params = {'backend': 'pdf',\n",
    "              'axes.titlesize': 8,\n",
    "              'axes.labelsize': 8,\n",
    "              'font.size': 8,\n",
    "              'legend.fontsize': 8,\n",
    "              'xtick.labelsize': 8,\n",
    "              'ytick.labelsize': 8,\n",
    "              'font.family': 'DejaVu Serif',\n",
    "              'font.serif': 'Computer Modern',\n",
    "              }\n",
    "    matplotlib.rcParams.update(params)\n",
    "\n",
    "\n",
    "def dice_similarity_coefficient(inter, union):\n",
    "    \"\"\"Computes the dice similarity coefficient.\n",
    "\n",
    "    Args:\n",
    "        inter (iterable): iterable of the intersections\n",
    "        union (iterable): iterable of the unions\n",
    "    \"\"\"\n",
    "    return 2 * sum(inter) / (sum(union) + sum(inter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/miglab/Echonet-Dynamic/EchoNet-Dynamic-Data\"\n",
    "tasks = [\"LargeFrame\", \"SmallFrame\", \"LargeTrace\", \"SmallTrace\"]\n",
    "frames=32\n",
    "period = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "mean, std = get_mean_and_std(Echo(root=data_dir, split=\"train\"))\n",
    "kwargs = {\"target_type\": tasks,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"length\": frames,\n",
    "            \"period\": period,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [33.66532  33.742973 33.911003] \n",
      "Standard Deviation : [50.45345  50.4825   50.614986]\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean : {mean} \\nStandard Deviation : {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "dataset[\"train\"] = Echo(root=data_dir, split=\"train\", **kwargs)\n",
    "dataset[\"val\"] = Echo(root=data_dir, split=\"val\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Model : DeeplabV3 with Resnet101 and MobilenetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miglab/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/miglab/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_mobilenet_v3_large-fc3c493d.pth\" to /home/miglab/.cache/torch/hub/checkpoints/deeplabv3_mobilenet_v3_large-fc3c493d.pth\n",
      "100%|██████████| 42.3M/42.3M [00:03<00:00, 11.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Conv2d(model.classifier[-1].in_channels, 1, kernel_size=model.classifier[-1].kernel_size)  # change number of outputs to 1\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-5)\n",
    "lr_step_period = math.inf\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, lr_step_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _video_collate_fn(x):\n",
    "    video, target = zip(*x)  \n",
    "    i = list(map(lambda t: t.shape[1], video))  \n",
    "    video = torch.as_tensor(np.swapaxes(np.concatenate(video, 1), 0, 1))\n",
    "    target = zip(*target)\n",
    "\n",
    "    return video, target, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_run_epoch(model, dataloader, train, optim, device):\n",
    "\n",
    "    total = 0.\n",
    "    n = 0\n",
    "\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    pos_pix = 0\n",
    "    neg_pix = 0\n",
    "\n",
    "    model.train(train)\n",
    "\n",
    "    large_inter = 0\n",
    "    large_union = 0\n",
    "    small_inter = 0\n",
    "    small_union = 0\n",
    "    large_inter_list = []\n",
    "    large_union_list = []\n",
    "    small_inter_list = []\n",
    "    small_union_list = []\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "            for (_, (large_frame, small_frame, large_trace, small_trace)) in dataloader:\n",
    "                # Count number of pixels in/out of human segmentation\n",
    "                pos += (large_trace == 1).sum().item()\n",
    "                pos += (small_trace == 1).sum().item()\n",
    "                neg += (large_trace == 0).sum().item()\n",
    "                neg += (small_trace == 0).sum().item()\n",
    "\n",
    "                # Count number of pixels in/out of computer segmentation\n",
    "                pos_pix += (large_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                pos_pix += (small_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (large_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (small_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "\n",
    "                # Run prediction for diastolic frames and compute loss\n",
    "                large_frame = large_frame.to(device)\n",
    "                large_trace = large_trace.to(device)\n",
    "                y_large = model(large_frame)[\"out\"]\n",
    "                loss_large = torch.nn.functional.binary_cross_entropy_with_logits(y_large[:, 0, :, :], large_trace, reduction=\"sum\")\n",
    "                # Compute pixel intersection and union between human and computer segmentations\n",
    "                large_inter += np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                large_union += np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                large_inter_list.extend(np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "                large_union_list.extend(np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                # Run prediction for systolic frames and compute loss\n",
    "                small_frame = small_frame.to(device)\n",
    "                small_trace = small_trace.to(device)\n",
    "                y_small = model(small_frame)[\"out\"]\n",
    "                loss_small = torch.nn.functional.binary_cross_entropy_with_logits(y_small[:, 0, :, :], small_trace, reduction=\"sum\")\n",
    "                # Compute pixel intersection and union between human and computer segmentations\n",
    "                small_inter += np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                small_union += np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum()\n",
    "                small_inter_list.extend(np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "                small_union_list.extend(np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace[:, :, :].detach().cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                # Take gradient step if training\n",
    "                loss = (loss_large + loss_small) / 2\n",
    "                if train:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                # Accumulate losses and compute baselines\n",
    "                total += loss.item()\n",
    "                n += large_trace.size(0)\n",
    "                p = pos / (pos + neg)\n",
    "                p_pix = (pos_pix + 1) / (pos_pix + neg_pix + 2)\n",
    "\n",
    "                # Show info on process bar\n",
    "                pbar.set_postfix_str(\"{:.4f} ({:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(total / n / 112 / 112, loss.item() / large_trace.size(0) / 112 / 112, -p * math.log(p) - (1 - p) * math.log(1 - p), (-p_pix * np.log(p_pix) - (1 - p_pix) * np.log(1 - p_pix)).mean(), 2 * large_inter / (large_union + large_inter), 2 * small_inter / (small_union + small_inter)))\n",
    "                pbar.update()\n",
    "\n",
    "    large_inter_list = np.array(large_inter_list)\n",
    "    large_union_list = np.array(large_union_list)\n",
    "    small_inter_list = np.array(small_inter_list)\n",
    "    small_union_list = np.array(small_union_list)\n",
    "\n",
    "    return (total / n / 112 / 112,\n",
    "            large_inter_list,\n",
    "            large_union_list,\n",
    "            small_inter_list,\n",
    "            small_union_list,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet_v3_large'\n",
    "pretrained = True\n",
    "num_epochs = 40 \n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "run_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seg = os.path.join(\"Models\", \"segmentation\", \"{}_{}\".format(model_name, \"pretrained\" if pretrained else \"random\"))\n",
    "os.makedirs(output_seg, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with open(os.path.join(output_seg, \"log.csv\"), \"a\") as f:\n",
    "    epoch_resume = 0\n",
    "    bestLoss = float(\"inf\")\n",
    "    try:\n",
    "        # Attempt to load checkpoint\n",
    "        checkpoint = torch.load(os.path.join(output_seg, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optim.load_state_dict(checkpoint['opt_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_dict'])\n",
    "        epoch_resume = checkpoint[\"epoch\"] + 1\n",
    "        bestLoss = checkpoint[\"best_loss\"]\n",
    "        f.write(\"Resuming from epoch {}\\n\".format(epoch_resume))\n",
    "    except FileNotFoundError:\n",
    "        f.write(\"Starting run from scratch\\n\")\n",
    "\n",
    "    for epoch in range(epoch_resume, num_epochs):\n",
    "        print(\"Epoch #{}\".format(epoch), flush=True)\n",
    "        for phase in ['train', 'val']:\n",
    "            start_time = time.time()\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.reset_peak_memory_stats(i)\n",
    "\n",
    "            ds = dataset[phase]\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=(device.type == \"cuda\"), drop_last=(phase == \"train\"))\n",
    "\n",
    "            loss, large_inter, large_union, small_inter, small_union = seg_run_epoch(model, dataloader, phase == \"train\", optim, device)\n",
    "            overall_dice = 2 * (large_inter.sum() + small_inter.sum()) / (large_union.sum() + large_inter.sum() + small_union.sum() + small_inter.sum())\n",
    "            large_dice = 2 * large_inter.sum() / (large_union.sum() + large_inter.sum())\n",
    "            small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(epoch,\n",
    "                                                                phase,\n",
    "                                                                loss,\n",
    "                                                                overall_dice,\n",
    "                                                                large_dice,\n",
    "                                                                small_dice,\n",
    "                                                                time.time() - start_time,\n",
    "                                                                large_inter.size,\n",
    "                                                                sum(torch.cuda.max_memory_allocated() for i in range(torch.cuda.device_count())),\n",
    "                                                                sum(torch.cuda.max_memory_reserved() for i in range(torch.cuda.device_count())),\n",
    "                                                                batch_size))\n",
    "            f.flush()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save checkpoint\n",
    "        save = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': bestLoss,\n",
    "            'loss': loss,\n",
    "            'opt_dict': optim.state_dict(),\n",
    "            'scheduler_dict': scheduler.state_dict(),\n",
    "        }\n",
    "        torch.save(save, os.path.join(output_seg, \"checkpoint.pt\"))\n",
    "        if loss < bestLoss:\n",
    "            torch.save(save, os.path.join(output_seg, \"best.pt\"))\n",
    "            bestLoss = loss\n",
    "\n",
    "    # Load best weights\n",
    "    if num_epochs != 0:\n",
    "        checkpoint = torch.load(os.path.join(output_seg, \"best.pt\"))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        f.write(\"Best validation loss {} from epoch {}\\n\".format(checkpoint[\"loss\"], checkpoint[\"epoch\"]))\n",
    "\n",
    "    if run_test:\n",
    "        # Run on validation and test\n",
    "        for split in [\"val\", \"test\"]:\n",
    "            dataset = Echo(root=data_dir, split=split, **kwargs)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                        batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=(device.type == \"cuda\"))\n",
    "            loss, large_inter, large_union, small_inter, small_union = seg_run_epoch(model, dataloader, False, None, device)\n",
    "\n",
    "            overall_dice = 2 * (large_inter + small_inter) / (large_union + large_inter + small_union + small_inter)\n",
    "            large_dice = 2 * large_inter / (large_union + large_inter)\n",
    "            small_dice = 2 * small_inter / (small_union + small_inter)\n",
    "            with open(os.path.join(output_seg, \"{}_dice.csv\".format(split)), \"w\") as g:\n",
    "                g.write(\"Filename, Overall, Large, Small\\n\")\n",
    "                for (filename, overall, large, small) in zip(dataset.fnames, overall_dice, large_dice, small_dice):\n",
    "                    g.write(\"{},{},{},{}\\n\".format(filename, overall, large, small))\n",
    "\n",
    "            f.write(\"{} dice (overall): {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(np.concatenate((large_inter, small_inter)), np.concatenate((large_union, small_union)), dice_similarity_coefficient)))\n",
    "            f.write(\"{} dice (large):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(large_inter, large_union, dice_similarity_coefficient)))\n",
    "            f.write(\"{} dice (small):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(small_inter, small_union, dice_similarity_coefficient)))\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face Intel DPT Large Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 23.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/miglab/Echonet-Dynamic/EchoNet-Dynamic-Data\"\n",
    "tasks = [\"LargeFrame\", \"SmallFrame\", \"LargeTrace\", \"SmallTrace\"]\n",
    "frames=32\n",
    "period = 2\n",
    "\n",
    "mean, std = get_mean_and_std(Echo(root=data_dir, split=\"train\"))\n",
    "kwargs = {\"target_type\": tasks,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"length\": frames,\n",
    "            \"period\": period,\n",
    "            }\n",
    "dataset = {}\n",
    "\n",
    "dataset[\"train\"] = Echo(root=data_dir, split=\"train\", **kwargs)\n",
    "dataset[\"val\"] = Echo(root=data_dir, split=\"val\", **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _video_collate_fn(x):\n",
    "    video, target = zip(*x)  \n",
    "    i = list(map(lambda t: t.shape[1], video))  \n",
    "    video = torch.as_tensor(np.swapaxes(np.concatenate(video, 1), 0, 1))\n",
    "    target = zip(*target)\n",
    "\n",
    "    return video, target, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miglab/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-28 15:13:09.033747: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 15:13:09.040111: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732786989.046767   33262 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732786989.048724   33262 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 15:13:09.056309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPTForSemanticSegmentation, DPTImageProcessor\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Load the DPT model and feature extractor\n",
    "feature_extractor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large-ade\")\n",
    "model = DPTForSemanticSegmentation.from_pretrained(\"Intel/dpt-large-ade\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-5)\n",
    "lr_step_period = math.inf\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, lr_step_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hf_seg_run_epoch(model, dataloader, train, optim, device):\n",
    "    total = 0.\n",
    "    n = 0\n",
    "\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    pos_pix = 0\n",
    "    neg_pix = 0\n",
    "\n",
    "    model.train(train)\n",
    "\n",
    "    large_inter = 0\n",
    "    large_union = 0\n",
    "    small_inter = 0\n",
    "    small_union = 0\n",
    "    large_inter_list = []\n",
    "    large_union_list = []\n",
    "    small_inter_list = []\n",
    "    small_union_list = []\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
    "            for (_, (large_frame, small_frame, large_trace, small_trace)) in dataloader:\n",
    "                pos += (large_trace == 1).sum().item()\n",
    "                pos += (small_trace == 1).sum().item()\n",
    "                neg += (large_trace == 0).sum().item()\n",
    "                neg += (small_trace == 0).sum().item()\n",
    "\n",
    "                pos_pix += (large_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                pos_pix += (small_trace == 1).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (large_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "                neg_pix += (small_trace == 0).sum(0).to(\"cpu\").detach().numpy()\n",
    "\n",
    "                large_frame = (large_frame - large_frame.min()) / (large_frame.max() - large_frame.min())\n",
    "                small_frame = (small_frame - small_frame.min()) / (small_frame.max() - small_frame.min())\n",
    "\n",
    "                # Preprocess frames for DPT model\n",
    "                large_frame = feature_extractor(images=large_frame.permute(0, 2, 3, 1).cpu().numpy(), return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "                small_frame = feature_extractor(images=small_frame.permute(0, 2, 3, 1).cpu().numpy(), return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "                \n",
    "                # Compute predictions\n",
    "                y_large = model(large_frame).logits\n",
    "                y_small = model(small_frame).logits\n",
    "\n",
    "                # Resize predictions to match ground truth size\n",
    "                y_large = torch.nn.functional.interpolate(y_large, size=large_trace.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                y_small = torch.nn.functional.interpolate(y_small, size=small_trace.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "                # Compute binary cross-entropy loss\n",
    "                loss_large = torch.nn.functional.binary_cross_entropy_with_logits(y_large[:, 0, :, :], large_trace.to(device), reduction=\"sum\")\n",
    "                loss_small = torch.nn.functional.binary_cross_entropy_with_logits(y_small[:, 0, :, :], small_trace.to(device), reduction=\"sum\")\n",
    "\n",
    "                # Intersection and union for large and small frames\n",
    "                large_inter += np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace.cpu().numpy() > 0.).sum()\n",
    "                large_union += np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace.cpu().numpy() > 0.).sum()\n",
    "                large_inter_list.extend(np.logical_and(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace.cpu().numpy() > 0.).sum((1, 2)))\n",
    "                large_union_list.extend(np.logical_or(y_large[:, 0, :, :].detach().cpu().numpy() > 0., large_trace.cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                small_inter += np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace.cpu().numpy() > 0.).sum()\n",
    "                small_union += np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace.cpu().numpy() > 0.).sum()\n",
    "                small_inter_list.extend(np.logical_and(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace.cpu().numpy() > 0.).sum((1, 2)))\n",
    "                small_union_list.extend(np.logical_or(y_small[:, 0, :, :].detach().cpu().numpy() > 0., small_trace.cpu().numpy() > 0.).sum((1, 2)))\n",
    "\n",
    "                # Take gradient step if training\n",
    "                loss = (loss_large + loss_small) / 2\n",
    "                if train:\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                total += loss.item()\n",
    "                n += large_trace.size(0)\n",
    "                p = pos / (pos + neg)\n",
    "                p_pix = (pos_pix + 1) / (pos_pix + neg_pix + 2)\n",
    "\n",
    "                pbar.set_postfix_str(\"{:.4f} ({:.4f}) / {:.4f} {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                    total / n / 112 / 112, \n",
    "                    loss.item() / large_trace.size(0) / 112 / 112, \n",
    "                    -p * math.log(p) - (1 - p) * math.log(1 - p), \n",
    "                    (-p_pix * np.log(p_pix) - (1 - p_pix) * np.log(1 - p_pix)).mean(), \n",
    "                    2 * large_inter / (large_union + large_inter), \n",
    "                    2 * small_inter / (small_union + small_inter)))\n",
    "                pbar.update()\n",
    "\n",
    "    large_inter_list = np.array(large_inter_list)\n",
    "    large_union_list = np.array(large_union_list)\n",
    "    small_inter_list = np.array(small_inter_list)\n",
    "    small_union_list = np.array(small_union_list)\n",
    "\n",
    "    return (total / n / 112 / 112,\n",
    "            large_inter_list,\n",
    "            large_union_list,\n",
    "            small_inter_list,\n",
    "            small_union_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'intel-dpt-large'\n",
    "pretrained = True\n",
    "num_epochs = 20\n",
    "batch_size = 2\n",
    "num_workers = 8\n",
    "run_test = True\n",
    "output_hf_seg = os.path.join(\"output\", \"segmentation\", \"{}_{}\".format(model_name, \"pretrained\" if pretrained else \"random\"))\n",
    "os.makedirs(output_hf_seg, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DPTForSemanticSegmentation were not initialized from the model checkpoint at Intel/dpt-large-ade and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.batch_norm1.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm1.weight', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.bias', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.num_batches_tracked', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_mean', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.running_var', 'neck.fusion_stage.layers.0.residual_layer1.batch_norm2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch #0\n",
      "/tmp/ipykernel_33262/3261743030.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(output_hf_seg, \"checkpoint.pt\"))\n",
      "  0%|          | 0/3730 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "100%|██████████| 3730/3730 [1:01:14<00:00,  1.02it/s, 0.2232 (0.0887) / 0.3135 0.1346, 0.3278, 0.3147]\n",
      "100%|██████████| 644/644 [03:44<00:00,  2.87it/s, 1.0014 (1.0426) / 0.3145 0.1385, 0.2365, 0.3582]\n",
      "Epoch #1\n",
      "100%|██████████| 3730/3730 [1:01:07<00:00,  1.02it/s, 0.1101 (0.1618) / 0.3135 0.1346, 0.7329, 0.7721]\n",
      "100%|██████████| 644/644 [03:43<00:00,  2.88it/s, 0.2192 (0.3164) / 0.3145 0.1385, 0.5950, 0.7446]\n",
      "Epoch #2\n",
      "100%|██████████| 3730/3730 [58:39<00:00,  1.06it/s, 0.0984 (0.0733) / 0.3135 0.1346, 0.7665, 0.7895]\n",
      "100%|██████████| 644/644 [03:43<00:00,  2.88it/s, 0.1287 (0.1045) / 0.3145 0.1385, 0.6894, 0.8004]\n",
      "Epoch #3\n",
      "100%|██████████| 3730/3730 [1:01:37<00:00,  1.01it/s, 0.0961 (0.0649) / 0.3135 0.1346, 0.7723, 0.7940]\n",
      "100%|██████████| 644/644 [03:55<00:00,  2.74it/s, 0.2552 (0.2243) / 0.3145 0.1385, 0.5639, 0.7239]\n",
      "Epoch #4\n",
      "100%|██████████| 3730/3730 [1:02:21<00:00,  1.00s/it, 0.0948 (0.1306) / 0.3135 0.1346, 0.7757, 0.7963]\n",
      "100%|██████████| 644/644 [03:55<00:00,  2.73it/s, 0.5719 (0.6296) / 0.3145 0.1385, 0.4029, 0.5727]\n",
      "Epoch #5\n",
      "100%|██████████| 3730/3730 [1:02:08<00:00,  1.00it/s, 0.0945 (0.0639) / 0.3135 0.1346, 0.7770, 0.7968]\n",
      "100%|██████████| 644/644 [03:57<00:00,  2.71it/s, 0.1423 (0.1453) / 0.3145 0.1385, 0.6803, 0.7997]\n",
      "Epoch #6\n",
      "100%|██████████| 3730/3730 [1:02:36<00:00,  1.01s/it, 0.0939 (0.0889) / 0.3135 0.1346, 0.7783, 0.7985]\n",
      "100%|██████████| 644/644 [03:58<00:00,  2.70it/s, 0.0981 (0.0804) / 0.3145 0.1385, 0.7615, 0.7719]\n",
      "Epoch #7\n",
      "100%|██████████| 3730/3730 [1:01:48<00:00,  1.01it/s, 0.0936 (0.0486) / 0.3135 0.1346, 0.7790, 0.7988]\n",
      "100%|██████████| 644/644 [03:54<00:00,  2.75it/s, 0.1114 (0.0926) / 0.3145 0.1385, 0.6846, 0.6850]\n",
      "Epoch #8\n",
      "100%|██████████| 3730/3730 [1:02:13<00:00,  1.00s/it, 0.0932 (0.0498) / 0.3135 0.1346, 0.7791, 0.8004]\n",
      "100%|██████████| 644/644 [03:57<00:00,  2.71it/s, 0.5061 (0.5115) / 0.3145 0.1385, 0.4410, 0.6241]\n",
      "Epoch #9\n",
      "100%|██████████| 3730/3730 [1:01:50<00:00,  1.01it/s, 0.0924 (0.1205) / 0.3135 0.1346, 0.7821, 0.8034]\n",
      "100%|██████████| 644/644 [03:54<00:00,  2.74it/s, 0.0907 (0.0627) / 0.3145 0.1385, 0.7840, 0.8031]\n",
      "Epoch #10\n",
      "100%|██████████| 3730/3730 [1:01:27<00:00,  1.01it/s, 0.0919 (0.1456) / 0.3135 0.1346, 0.7827, 0.8043]\n",
      "100%|██████████| 644/644 [03:54<00:00,  2.75it/s, 0.1219 (0.0867) / 0.3145 0.1385, 0.7133, 0.8190]\n",
      "Epoch #11\n",
      "100%|██████████| 3730/3730 [1:01:53<00:00,  1.00it/s, 0.0925 (0.0703) / 0.3135 0.1346, 0.7818, 0.8031]\n",
      "100%|██████████| 644/644 [03:55<00:00,  2.74it/s, 0.1223 (0.1361) / 0.3145 0.1385, 0.7097, 0.8187]\n",
      "Epoch #12\n",
      "100%|██████████| 3730/3730 [1:01:50<00:00,  1.01it/s, 0.0925 (0.0882) / 0.3135 0.1346, 0.7823, 0.8035]\n",
      "100%|██████████| 644/644 [03:54<00:00,  2.74it/s, 0.1620 (0.1975) / 0.3145 0.1385, 0.6628, 0.7977]\n",
      "Epoch #13\n",
      "100%|██████████| 3730/3730 [1:01:35<00:00,  1.01it/s, 0.0917 (0.0939) / 0.3135 0.1346, 0.7837, 0.8045]\n",
      "100%|██████████| 644/644 [03:53<00:00,  2.76it/s, 0.1071 (0.1129) / 0.3145 0.1385, 0.7473, 0.8319]\n",
      "Epoch #14\n",
      "100%|██████████| 3730/3730 [1:01:52<00:00,  1.00it/s, 0.0915 (0.1293) / 0.3135 0.1346, 0.7845, 0.8054]\n",
      "100%|██████████| 644/644 [03:55<00:00,  2.73it/s, 1.4038 (1.4023) / 0.3145 0.1385, 0.2573, 0.3934]\n",
      "Epoch #15\n",
      "100%|██████████| 3730/3730 [1:01:34<00:00,  1.01it/s, 0.0909 (0.0618) / 0.3135 0.1346, 0.7858, 0.8069]\n",
      "100%|██████████| 644/644 [03:53<00:00,  2.76it/s, 0.1069 (0.0614) / 0.3145 0.1385, 0.7257, 0.7053]\n",
      "Epoch #16\n",
      "100%|██████████| 3730/3730 [1:02:14<00:00,  1.00s/it, 0.0907 (0.0739) / 0.3135 0.1346, 0.7872, 0.8075]\n",
      "100%|██████████| 644/644 [03:59<00:00,  2.69it/s, 1.4838 (1.3215) / 0.3145 0.1385, 0.2744, 0.4277]\n",
      "Epoch #17\n",
      "100%|██████████| 3730/3730 [1:02:31<00:00,  1.01s/it, 0.0909 (0.0638) / 0.3135 0.1346, 0.7864, 0.8077]\n",
      "100%|██████████| 644/644 [03:56<00:00,  2.72it/s, 0.1082 (0.1331) / 0.3145 0.1385, 0.7493, 0.8323]\n",
      "Epoch #18\n",
      "100%|██████████| 3730/3730 [1:01:48<00:00,  1.01it/s, 0.0910 (0.0682) / 0.3135 0.1346, 0.7861, 0.8071]\n",
      "100%|██████████| 644/644 [03:43<00:00,  2.88it/s, 0.2070 (0.1924) / 0.3145 0.1385, 0.1716, 0.2174]\n",
      "Epoch #19\n",
      "100%|██████████| 3730/3730 [58:41<00:00,  1.06it/s, 0.0905 (0.0732) / 0.3135 0.1346, 0.7878, 0.8080]\n",
      "100%|██████████| 644/644 [03:43<00:00,  2.88it/s, 0.1420 (0.0648) / 0.3145 0.1385, 0.5708, 0.5585]\n",
      "/tmp/ipykernel_33262/3261743030.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(output_hf_seg, \"best.pt\"))\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "with open(os.path.join(output_hf_seg, \"log.csv\"), \"a\") as f:\n",
    "    epoch_resume = 0\n",
    "    bestLoss = float(\"inf\")\n",
    "    try:\n",
    "        # Attempt to load checkpoint\n",
    "        checkpoint = torch.load(os.path.join(output_hf_seg, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optim.load_state_dict(checkpoint['opt_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_dict'])\n",
    "        epoch_resume = checkpoint[\"epoch\"] + 1\n",
    "        bestLoss = checkpoint[\"best_loss\"]\n",
    "        f.write(\"Resuming from epoch {}\\n\".format(epoch_resume))\n",
    "    except FileNotFoundError:\n",
    "        f.write(\"Starting run from scratch\\n\")\n",
    "\n",
    "    for epoch in range(epoch_resume, num_epochs):\n",
    "        print(\"Epoch #{}\".format(epoch), flush=True)\n",
    "        for phase in ['train', 'val']:\n",
    "            start_time = time.time()\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.reset_peak_memory_stats(i)\n",
    "\n",
    "            ds = dataset[phase]\n",
    "            dataloader = torch.utils.data.DataLoader(\n",
    "                ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=(device.type == \"cuda\"), drop_last=(phase == \"train\"))\n",
    "\n",
    "            loss, large_inter, large_union, small_inter, small_union = hf_seg_run_epoch(model, dataloader, phase == \"train\", optim, device)\n",
    "            overall_dice = 2 * (large_inter.sum() + small_inter.sum()) / (large_union.sum() + large_inter.sum() + small_union.sum() + small_inter.sum())\n",
    "            large_dice = 2 * large_inter.sum() / (large_union.sum() + large_inter.sum())\n",
    "            small_dice = 2 * small_inter.sum() / (small_union.sum() + small_inter.sum())\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(epoch,\n",
    "                                                                phase,\n",
    "                                                                loss,\n",
    "                                                                overall_dice,\n",
    "                                                                large_dice,\n",
    "                                                                small_dice,\n",
    "                                                                time.time() - start_time,\n",
    "                                                                large_inter.size,\n",
    "                                                                sum(torch.cuda.max_memory_allocated() for i in range(torch.cuda.device_count())),\n",
    "                                                                sum(torch.cuda.max_memory_reserved() for i in range(torch.cuda.device_count())),\n",
    "                                                                batch_size))\n",
    "            f.flush()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save checkpoint\n",
    "        save = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': bestLoss,\n",
    "            'loss': loss,\n",
    "            'opt_dict': optim.state_dict(),\n",
    "            'scheduler_dict': scheduler.state_dict(),\n",
    "        }\n",
    "        torch.save(save, os.path.join(output_hf_seg, \"checkpoint.pt\"))\n",
    "        if loss < bestLoss:\n",
    "            torch.save(save, os.path.join(output_hf_seg, \"best.pt\"))\n",
    "            bestLoss = loss\n",
    "\n",
    "    # Load best weights\n",
    "    if num_epochs != 0:\n",
    "        checkpoint = torch.load(os.path.join(output_hf_seg, \"best.pt\"))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        f.write(\"Best validation loss {} from epoch {}\\n\".format(checkpoint[\"loss\"], checkpoint[\"epoch\"]))\n",
    "\n",
    "    if run_test:\n",
    "        # Run on validation and test\n",
    "        for split in [\"val\", \"test\"]:\n",
    "            dataset = Echo(root=data_dir, split=split, **kwargs)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                        batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=(device.type == \"cuda\"))\n",
    "            loss, large_inter, large_union, small_inter, small_union = seg_run_epoch(model, dataloader, False, None, device)\n",
    "\n",
    "            overall_dice = 2 * (large_inter + small_inter) / (large_union + large_inter + small_union + small_inter)\n",
    "            large_dice = 2 * large_inter / (large_union + large_inter)\n",
    "            small_dice = 2 * small_inter / (small_union + small_inter)\n",
    "            with open(os.path.join(output_hf_seg, \"{}_dice.csv\".format(split)), \"w\") as g:\n",
    "                g.write(\"Filename, Overall, Large, Small\\n\")\n",
    "                for (filename, overall, large, small) in zip(dataset.fnames, overall_dice, large_dice, small_dice):\n",
    "                    g.write(\"{},{},{},{}\\n\".format(filename, overall, large, small))\n",
    "\n",
    "            f.write(\"{} dice (overall): {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(np.concatenate((large_inter, small_inter)), np.concatenate((large_union, small_union)), dice_similarity_coefficient)))\n",
    "            f.write(\"{} dice (large):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(large_inter, large_union, dice_similarity_coefficient)))\n",
    "            f.write(\"{} dice (small):   {:.4f} ({:.4f} - {:.4f})\\n\".format(split, *bootstrap(small_inter, small_union, dice_similarity_coefficient)))\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
